{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to examine rag evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.append(\"../dev/\")\n",
    "sys.path.append(\"../src/\")\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from csv_helpers import get_csv_files_from_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse and plot the pool results - of all entangled parameter permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition\n",
    "# Look at the eval results files, especially the mean correctness\n",
    "import dask.dataframe as dd\n",
    "# Loop over all eval results files\n",
    "# Get column names which are the config parameters and the mean correctness or some other metric\n",
    "def create_df_mean_correctness(dataset,evaluator, directory):\n",
    "    columns = [\n",
    "        \"quExp\",\n",
    "        \"rerank\",\n",
    "        \"cExp\",\n",
    "        \"backRev\",\n",
    "        \"numRefLim\",\n",
    "        \"select\",\n",
    "        \"evaluator\",\n",
    "    ] + [\"MeanCorrectness\"]\n",
    "    mean_correctness_df = pd.DataFrame(columns=columns)\n",
    "    # Get file names\n",
    "    eval_results_file_names = get_csv_files_from_dir(directory)\n",
    "    # Iterate over all eval results files\n",
    "    for filename in eval_results_file_names[:]:  # Iterate over all eval results files\n",
    "        # Filter files: Only look at files with quExp1_rerank1_cExp*_backRevFalse_numRef4\n",
    "        # first_file = f\"{eval_results_dir}/{eval_results_file_names[1]}\"  # Slice for dev\n",
    "        file = eval_results_dir + \"/\" + filename  # Slice for dev\n",
    "    \n",
    "        # Read eval results from CSV\n",
    "        eval_results_df = dd.read_csv(file)\n",
    "        # Show the first 5 rows\n",
    "        \n",
    "        correctness_values = eval_results_df[\"Correct\"].dropna()\n",
    "        correctness_values = correctness_values.compute(engine=\"python\")\n",
    "    \n",
    "        #print(f\"Correctness values: {correctness_values}\")\n",
    "    \n",
    "        mean = sum(correctness_values) / len(correctness_values)\n",
    "        \n",
    "        # Project the values to be between 0 and 1\n",
    "        mean_scaled = (mean - min(correctness_values)) / (max(correctness_values) - min(correctness_values))\n",
    "\n",
    "        #print(f\"Mean correctness: {mean}\")\n",
    "        # Append the correctness values to the list\n",
    "        config = filename.split(\"_\")\n",
    "        #print(f\" Config: {config}\")\n",
    "        mean_correctness_df = pd.concat(\n",
    "            [\n",
    "                mean_correctness_df,\n",
    "                pd.DataFrame(\n",
    "                    [\n",
    "                        {\n",
    "                            \"quExp\": config[0].replace(\"quExp\", \"\"),\n",
    "                            \"rerank\": config[1].replace(\"rerank\", \"\"),\n",
    "                            \"cExp\": config[2].replace(\"cExp\", \"\"),\n",
    "                            \"backRev\": config[3].replace(\"backRev\", \"\"),\n",
    "                            \"numRefLim\": config[4].replace(\"numRefLim\", \"\"),\n",
    "                            \"select\": config[-2],\n",
    "                            \"evaluator\": config[-1].replace(\".csv\", \"\"),\n",
    "                            \"MeanCorrectness\": mean_scaled,\n",
    "                        }\n",
    "                    ]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    return mean_correctness_df\n",
    "\n",
    "\n",
    "def create_df_mean_correctness_single_param(dataset,evaluator, directory):\n",
    "    columns = [\n",
    "        \"quExp\",\n",
    "        \"rerank\",\n",
    "        \"cExp\",\n",
    "        \"backRev\",\n",
    "        \"numRefLim\",\n",
    "        \"llm\",\n",
    "        \"select\",\n",
    "        \"evaluator\",\n",
    "    ] + [\"MeanCorrectness\"]\n",
    "    mean_correctness_df = pd.DataFrame(columns=columns)\n",
    "    # Get file names\n",
    "    eval_results_file_names = get_csv_files_from_dir(directory)\n",
    "    # Iterate over all eval results files\n",
    "    for filename in eval_results_file_names[:]:  # Iterate over all eval results files\n",
    "        # Filter files: Only look at files with quExp1_rerank1_cExp*_backRevFalse_numRef4\n",
    "        # first_file = f\"{eval_results_dir}/{eval_results_file_names[1]}\"  # Slice for dev\n",
    "        file = eval_results_dir + \"/\" + filename  # Slice for dev\n",
    "    \n",
    "        # Read eval results from CSV\n",
    "        eval_results_df = dd.read_csv(file)\n",
    "        # Show the first 5 rows\n",
    "        \n",
    "        correctness_values = eval_results_df[\"Correct\"].dropna()\n",
    "        correctness_values = correctness_values.compute(engine=\"python\")\n",
    "    \n",
    "        #print(f\"Correctness values: {correctness_values}\")\n",
    "    \n",
    "        mean = sum(correctness_values) / len(correctness_values)\n",
    "        \n",
    "        # Project the values to be between 0 and 1\n",
    "        mean_scaled = (mean - min(correctness_values)) / (max(correctness_values) - min(correctness_values))\n",
    "\n",
    "        #print(f\"Mean correctness: {mean}\")\n",
    "        # Append the correctness values to the list\n",
    "        config = filename.split(\"_\")\n",
    "        #print(f\" Config: {config}\")\n",
    "        mean_correctness_df = pd.concat(\n",
    "            [\n",
    "                mean_correctness_df,\n",
    "                pd.DataFrame(\n",
    "                    [\n",
    "                        {\n",
    "                            \"quExp\": config[0].replace(\"quExp\", \"\"),\n",
    "                            \"rerank\": config[1].replace(\"rerank\", \"\"),\n",
    "                            \"cExp\": config[2].replace(\"cExp\", \"\"),\n",
    "                            \"backRev\": config[3].replace(\"backRev\", \"\"),\n",
    "                            \"numRefLim\": config[4].replace(\"numRefLim\", \"\"),\n",
    "                            \"llm\": config[5].replace(\":latest\", \"\"),\n",
    "                            \"select\": config[-2],\n",
    "                            \"evaluator\": config[-1].replace(\".csv\", \"\"),\n",
    "                            \"MeanCorrectness\": mean_scaled,\n",
    "                        }\n",
    "                    ]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    return mean_correctness_df\n",
    "\n",
    "\n",
    "# Rename columns and row entries for plotting\n",
    "def rename_for_plotting(dataframe): \n",
    "    new_dataframe = dataframe.rename(columns={\n",
    "        \"quExp\": \"Query Expansion\",\n",
    "        \"rerank\": \"Re-rank\",\n",
    "        \"cExp\": \"Context Expansion\",\n",
    "        \"backRev\": \"Backward Revision\",\n",
    "        \"numRefLim\": \"Number Sources\",\n",
    "        \"select\": \"Selection Method\",\n",
    "        \"evaluator\": \"Evaluator\",\n",
    "        \"MeanCorrectness\": \"Mean Correctness\"\n",
    "    }, inplace=False)\n",
    "\n",
    "    # Define custom names for the values in the 'Category' column\n",
    "    rerank_names = {\n",
    "        'False': 'Off',\n",
    "        'True': 'Semantic',\n",
    "        'rrf': 'RRF'\n",
    "    }\n",
    "    cExp_names = {\n",
    "        'False': 'Off',\n",
    "        'True': 'On',\n",
    "    }\n",
    "\n",
    "    queExp_names = {\n",
    "        '1': 'Off',\n",
    "        '2': '2',\n",
    "        '3': '3',\n",
    "    }\n",
    "\n",
    "    # Replace the values in the re-rank, qExp and cExp columns with the custom names\n",
    "    new_dataframe['Re-rank'] = new_dataframe['Re-rank'].map(rerank_names)\n",
    "    new_dataframe['Context Expansion'] = new_dataframe['Context Expansion'].map(cExp_names)\n",
    "    new_dataframe['Query Expansion'] = new_dataframe['Query Expansion'].map(queExp_names)\n",
    "\n",
    "    return new_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse parameter significance - ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results wo get insights on parameter significance\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")  # Suitable for colorblind readers\n",
    "sns.color_palette(\"muted\")    # Balanced colors\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "\n",
    "# Set configuration\n",
    "parameter = \"pool\" # pool, quExp, rerank, cExp, backRev, numRefLim\n",
    "evaluator = \"ROUGE-1\" # llm-judge, ROUGE-1\n",
    "dataset = \"miniWiki\" # miniBiosQA, miniWiki\n",
    "eval_results_dir = f\"../100_rows_eval_results/{dataset}/{parameter}/{evaluator}\"\n",
    "\n",
    "# Create the dataframe\n",
    "mean_correctness_df = create_df_mean_correctness(dataset,evaluator, eval_results_dir)\n",
    "mean_correctness_df = mean_correctness_df[mean_correctness_df[\"select\"] == \"correctness\"]\n",
    "# mean_correctness_df = mean_correctness_df[mean_correctness_df[\"rerank\"].isin([\"True\", \"rrf\"])] \n",
    "mean_correctness_df.sort_values(by=\"MeanCorrectness\", ascending=False)\n",
    "print(f\"Dataset: {dataset}, \\nEvaluator: {evaluator}, \\nParameter: {parameter}\")\n",
    "\n",
    "# Initialize pandas df to store p values and respective method\n",
    "p_values_df = pd.DataFrame(columns=[\"Method\", \"p-value\", \"f-value\"], index=None)\n",
    "# Loop over all rag methods\n",
    "\n",
    "mean_correctness_df = rename_for_plotting(mean_correctness_df)\n",
    "for rag_method in [\"Number Sources\", \"Query Expansion\", \"Re-rank\", \"Context Expansion\"]:\n",
    "    # Calcualate the p and f values\n",
    "    # Shorten the dataframe name\n",
    "    df = mean_correctness_df\n",
    "    #Get unique values of the rag method column \n",
    "    method_variables = df[rag_method].unique()\n",
    "    print(f\"Unique values of {rag_method}: {method_variables}\")\n",
    "    # One-way ANOVA: Test if the means of 'x' differ across categories in 'a'\n",
    "    groups = [df[df[rag_method] == variable]['Mean Correctness'] for variable in method_variables]\n",
    "    f_stat, p_value = stats.f_oneway(*groups)\n",
    "    print(f\"ANOVA result for {rag_method}: F-statistic = {f_stat}, p-value = {p_value}\")\n",
    "    # Append the p and f values to the dataframe\n",
    "    p_values_df = pd.concat(\n",
    "        [\n",
    "            p_values_df,\n",
    "            pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"Method\": rag_method,\n",
    "                        \"p-value\": p_value,\n",
    "                        \"f-value\": f_stat\n",
    "                    }\n",
    "                ]\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "   \n",
    "    # Make a histplot with respect to the rag method\n",
    "    hist = sns.histplot(data=mean_correctness_df, x=\"Mean Correctness\", hue=rag_method, kde=True)\n",
    "    \n",
    "    # Change labels of rag method categories\n",
    "    if rag_method == \"Re-rank\":\n",
    "        new_labels = [\"Off\",\"Semantic\",\"RRF\"]\n",
    "        for t, label in zip(hist.legend_.texts, new_labels):\n",
    "            t.set_text(label)  # Update the legend labels\n",
    "\n",
    "    if rag_method == \"Context Expansion\":\n",
    "        new_labels = [\"Off\",\"On\"]\n",
    "        for t, label in zip(hist.legend_.texts, new_labels):\n",
    "            t.set_text(label)  # Update the legend labels\n",
    "        \n",
    "    # plt.title(f\"Mean Correctness distribution with {evaluator} {dataset}\")\n",
    "    plt.savefig(f\"../plots/{dataset}_{evaluator}_{parameter}_{rag_method}_histplot_test.pdf\",format=\"pdf\")\n",
    "    plt.show()\n",
    " \n",
    "## Save the p values to a csv file\n",
    "#p_values_df.to_csv(f\"../plots/{dataset}_{evaluator}_{parameter}_p_values.csv\")\n",
    "#with pd.option_context(\"max_colwidth\", 100):\n",
    "#    p_values_df.to_latex(f\"../tables/{dataset}_{evaluator}_{parameter}_p_values.tex\",index=False)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot grid search results over all permuations - barplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results wo get insights on parameter significance\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")  # Suitable for colorblind readers\n",
    "#sns.color_palette(\"muted\")    # Balanced colors\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "\n",
    "#Iterate over methods , then iterate over dataset. Goal: 4 subplots for each method in a single plot\n",
    "for rag_method in [\"Number Sources\", \"Query Expansion\", \"Re-rank\", \"Context Expansion\"]:\n",
    "    # Set configuration\n",
    "    parameter = \"pool\" # pool, quExp, rerank, cExp, backRev, numRefLim\n",
    "    # Initialze plots object to fill with subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(4*3,3*3))\n",
    "    # Iterate over all datasets\n",
    "    for dataset in [\"miniWiki\", \"miniBiosQA\"]:\n",
    "        # Iterate over all evaluators\n",
    "        for evaluator in [\"ROUGE-1\", \"llm-judge\"]:\n",
    "            # Print for sanity check\n",
    "            print(f\"Dataset: {dataset}, \\nEvaluator: {evaluator}, \\nParameter: {parameter}, \\nRag method: {rag_method}\")\n",
    "\n",
    "            # Specify the directory\n",
    "            eval_results_dir = f\"../100_rows_eval_results/{dataset}/{parameter}/{evaluator}\"\n",
    "\n",
    "            # Create the dataframe\n",
    "            mean_correctness_df = create_df_mean_correctness(dataset,evaluator, eval_results_dir)\n",
    "\n",
    "            # Select only the correctness values as y values, ignore cr, faithfulness and ar values\n",
    "            mean_correctness_df = mean_correctness_df[mean_correctness_df[\"select\"] == \"correctness\"]\n",
    "\n",
    "            # Sort the values in descending order\n",
    "            mean_correctness_df.sort_values(by=\"MeanCorrectness\", ascending=False)\n",
    "           \n",
    "            # Re-name columns and row entries for plotting\n",
    "            mean_correctness_df = rename_for_plotting(mean_correctness_df)\n",
    "\n",
    "            #Get unique values of the rag method column \n",
    "            method_variables = mean_correctness_df[rag_method].unique()\n",
    "            print(f\"Unique values of {rag_method}: {method_variables}\")\n",
    "\n",
    "            # Fill the subplots with the histplots\n",
    "            \n",
    "            # Determine the position of the subplot\n",
    "            if dataset == \"miniWiki\" and evaluator == \"ROUGE-1\":\n",
    "                ax = axs[0, 0]\n",
    "                dataset_name, evaluator_name = \"Mini-Wiki\", \"Rouge-1\"\n",
    "            elif dataset == \"miniWiki\" and evaluator == \"llm-judge\":\n",
    "                ax = axs[0, 1]\n",
    "                dataset_name, evaluator_name = \"Mini-Wiki\", \"LLM-Judge\"\n",
    "            elif dataset == \"miniBiosQA\" and evaluator == \"ROUGE-1\":\n",
    "                ax = axs[1, 0]\n",
    "                dataset_name, evaluator_name = \"Mini-BioASQ\", \"Rouge-1\"\n",
    "            elif dataset == \"miniBiosQA\" and evaluator == \"llm-judge\":\n",
    "                ax = axs[1, 1]\n",
    "                dataset_name, evaluator_name = \"Mini-BioASQ\", \"LLM-Judge\"\n",
    "\n",
    "            # Create the histplot in the determined subplot\n",
    "            sns.histplot(data=mean_correctness_df, x=\"Mean Correctness\", hue=rag_method, kde=False, multiple=\"dodge\", palette=\"colorblind\", ax=ax)\n",
    "            sns.despine()\n",
    "            ax.set_title(f\"{dataset_name},  {evaluator_name}\", fontsize=12)\n",
    "            ax.set_xlabel(\"\")  # Remove individual x-axis label\n",
    "            ax.set_ylabel(\"\")  # Remove individual y-axis label\n",
    "    # Set combined x and y axis title\n",
    "    fig.supxlabel(\"Mean Correctness\")\n",
    "    fig.supylabel(\"Frequency\")\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # plt.title(f\"Mean Correctness distribution with {evaluator} {dataset}\")\n",
    "    plt.savefig(f\"../plots/Allpermuations_{rag_method}_histplot.pdf\",format=\"pdf\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Depth Analysis of single parameter - LLM, Rerank, Context Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results wo get insights on parameter significance\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")  # Suitable for colorblind readers\n",
    "#sns.color_palette(\"muted\")    # Balanced colors\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "\n",
    "#Iterate over methods , then iterate over dataset. Goal: 4 subplots for each method in a single plot\n",
    "for rag_method in [\"Re-rank\"]:  #[\"Number Sources\", \"Query Expansion\", \"Re-rank\", \"Context Expansion\"]:\n",
    "    # Set configuration\n",
    "    parameter = \"LLMs\" # pool, quExp, rerank, cExp, backRev, numRefLim\n",
    "    # Set dataset\n",
    "    dataset = \"miniBiosQA\" # miniBiosQA, miniWiki\n",
    "    # Initialze plots object to fill with subplots\n",
    "    fig, axs = plt.subplots(1,2, figsize=(4*3,2*3))\n",
    "    # Iterate over all evaluators\n",
    "    for evaluator in [\"ROUGE-1\", \"llm-judge\"]:\n",
    "        # Print for sanity check\n",
    "        print(f\"Dataset: {dataset}, \\nEvaluator: {evaluator}, \\nParameter: {parameter}, \\nRag method: {rag_method}\")\n",
    "        # Specify the directory\n",
    "        eval_results_dir = f\"../eval_results/{dataset}/{parameter}/{evaluator}\" # f\"../100_rows_eval_results/{dataset}/{parameter}/{evaluator}\" \n",
    "\n",
    "        # Create the dataframe\n",
    "        mean_correctness_df = create_df_mean_correctness_single_param(dataset,evaluator,eval_results_dir)\n",
    "        # Filter select is correctness\n",
    "        #mean_correctness_df = mean_correctness_df[mean_correctness_df[\"select\"] == \"correctness\"]\n",
    "        # Filter number of source \n",
    "        #mean_correctness_df = mean_correctness_df[mean_correctness_df[\"numRefLim\"].isin([\"3\"])]\n",
    "        # Filter quExp\n",
    "        #mean_correctness_df = mean_correctness_df[mean_correctness_df[\"quExp\"] == \"1\"]\n",
    "        # Filter cExp \n",
    "        #mean_correctness_df = mean_correctness_df[mean_correctness_df[\"cExp\"] == \"False\"]\n",
    "        \n",
    "\n",
    "        # Sort the values in descending order\n",
    "        mean_correctness_df.sort_values(by=\"MeanCorrectness\", ascending=False)\n",
    "\n",
    "        # Re-name columns and row entries for plotting\n",
    "        mean_correctness_df = rename_for_plotting(mean_correctness_df)\n",
    "        print(mean_correctness_df)\n",
    "\n",
    "        #Get unique values of the rag method column \n",
    "        method_variables = mean_correctness_df[rag_method].unique()\n",
    "        print(f\"Unique values of {rag_method}: {method_variables}\")\n",
    "\n",
    "        # Determine the position of the subplot\n",
    "       \n",
    "        if dataset == \"miniBiosQA\" and evaluator == \"ROUGE-1\":\n",
    "            ax = axs[0]\n",
    "            dataset_name, evaluator_name = \"Mini-BioASQ\", \"Rouge-1\"\n",
    "        elif dataset == \"miniBiosQA\" and evaluator == \"llm-judge\":\n",
    "            ax = axs[1]\n",
    "            dataset_name, evaluator_name = \"Mini-BioASQ\", \"LLM-Judge\"\n",
    "             # Set y axis limit\n",
    "            ax.set_ylim(0.8,1)\n",
    "\n",
    "        # Create the histplot in the determined subplot\n",
    "        sns.barplot(data=mean_correctness_df, x=\"llm\", y=\"Mean Correctness\",palette=\"colorblind\",ax=ax)\n",
    "        sns.despine()\n",
    "        ax.set_title(f\"{dataset_name},  {evaluator_name}\", fontsize=12)\n",
    "        ax.set_xlabel(\"\")  # Remove individual x-axis label\n",
    "        ax.set_ylabel(\"\")  # Remove individual y-axis label\n",
    "    \n",
    "    # Set combined x and y axis title\n",
    "    fig.supxlabel(\"LLMs\")\n",
    "    fig.supylabel(\"Correctness\")\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    # plt.title(f\"Mean Correctness distribution with {evaluator} {dataset}\")\n",
    "    plt.savefig(f\"../plots/SingleParam_LLMs_barplot.pdf\",format=\"pdf\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make large table of correctnes values for all parameter configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results wo get insights on parameter significance\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "# Set configuration\n",
    "parameter = \"pool\" # pool, quExp, rerank, cExp, backRev, numRefLim\n",
    "evaluator = \"ROUGE-1\" # llm-judge, ROUGE-1\n",
    "dataset = \"miniBiosQA\" # miniBiosQA, miniWiki\n",
    "eval_results_dir = f\"../100_rows_eval_results/{dataset}/{parameter}/{evaluator}\"\n",
    "\n",
    "# Create the dataframe\n",
    "mean_correctness_df = create_df_mean_correctness(dataset,evaluator, eval_results_dir)\n",
    "# Rename the columns of the dataframe for plotting\n",
    "\n",
    "\n",
    "mean_correctness_df = rename_for_plotting(mean_correctness_df)\n",
    "\n",
    "\n",
    "\n",
    "mean_correctness_df = mean_correctness_df[mean_correctness_df[\"Selection Method\"] == \"correctness\"]\n",
    "mean_correctness_df = mean_correctness_df[mean_correctness_df[\"Backward Revision\"] == \"False\"]\n",
    "mean_correctness_df = mean_correctness_df[['Query Expansion', 'Re-rank', 'Context Expansion', 'Number Sources', 'Mean Correctness']]\n",
    "mean_correctness_df = mean_correctness_df.sort_values(by=\"Mean Correctness\", ascending=False)\n",
    "table = mean_correctness_df.to_latex(f\"../tables/{dataset}_{evaluator}_{parameter}_full_correctness_table.tex\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate gold passages if given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_goldPassages(dataset,directory):\n",
    "    # Column names with the config params and the matches and sum of matches\n",
    "    columns = [\n",
    "        \"quExp\",\n",
    "        \"rerank\",\n",
    "        \"cExp\",\n",
    "        \"backRev\",\n",
    "        \"numRefLim\",\n",
    "    ] + [\"matches\", \"sum_matches\"]\n",
    "    context_id_matches = pd.DataFrame(columns=columns)\n",
    "    # Get file names\n",
    "    eval_results_file_names = get_csv_files_from_dir(directory)\n",
    "    # Iterate over all eval results files\n",
    "    for filename in eval_results_file_names[:]:  # Iterate over all eval results files\n",
    "        # Filter files: Only look at files with quExp1_rerank1_cExp*_backRevFalse_numRef4\n",
    "        # first_file = f\"{eval_results_dir}/{eval_results_file_names[1]}\"  # Slice for dev\n",
    "        file = directory + \"/\" + filename  # Slice for dev\n",
    "    \n",
    "        # Read eval results from CSV\n",
    "        df = pd.read_csv(file)\n",
    "        # Keep only the 'contexts_ids' and 'goldPassages' columns\n",
    "        # Calcualte matches between the ids in the two columns\n",
    "      \n",
    "        # Account for a row containint no list but a single int\n",
    "        if isinstance(df[\"contexts_ids\"][0], int):\n",
    "            df[\"contexts_ids\"] = df[\"contexts_ids\"].apply(lambda row: [row])\n",
    "        else:\n",
    "            df[\"contexts_ids\"] = df[\"contexts_ids\"].apply(\n",
    "                lambda row: [int(row)] if isinstance(row, int) else list(map(int, row.split(\", \"))) if isinstance(row, str) and row else []\n",
    "            )\n",
    "       \n",
    "        df[\"goldPassages\"] = df[\"goldPassages\"].apply(\n",
    "            lambda row: list(map(int, row.split(\", \"))) if row else []\n",
    "        )\n",
    "        \n",
    "        # Calculate matches between the ids in the two columns\n",
    "        matches = df.apply(\n",
    "            lambda row: len(set(row[\"contexts_ids\"]).intersection(row[\"goldPassages\"])),\n",
    "            axis=1,\n",
    "        )\n",
    "        # Calculate the recall , i.e. the number of matches divided by the number of relevant documents. Note that the number of relevant documents is the number of gold passages and varies per question\n",
    "        recall = df.apply(\n",
    "            lambda row: len(set(row[\"contexts_ids\"]).intersection(row[\"goldPassages\"])) / len(row[\"goldPassages\"]) if len(row[\"goldPassages\"]) > 0 else 0,\n",
    "            axis=1,\n",
    "        )\n",
    "        \n",
    "        numberGoldPassages = df.apply(\n",
    "            lambda row: len(row[\"goldPassages\"]),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "\n",
    "        # Add the matches to the DataFrame\n",
    "        df = df[[\"contexts_ids\", \"goldPassages\"]]\n",
    "        # df[\"matches\"] = matches\n",
    "\n",
    "        # Write the eval results to a csv file\n",
    "        # eval_results_dir = \"./parallel_100_rows_eval\"\n",
    "        # Add param settings and matches to the DataFrame\n",
    "        # print(f\"Param settings: {param_settings}\")\n",
    "        # print(f\"Matches \\n: {matches.array}\")\n",
    "        # Add the param settings and matches row per row to the DataFrame\n",
    "        config = filename.split(\"_\")\n",
    "        context_id_matches = pd.concat(\n",
    "            [\n",
    "                context_id_matches,\n",
    "                pd.DataFrame(\n",
    "                    [\n",
    "                        {\n",
    "                            \"quExp\": config[0].replace(\"quExp\", \"\"),\n",
    "                            \"rerank\": config[1].replace(\"rerank\", \"\"),\n",
    "                            \"cExp\": config[2].replace(\"cExp\", \"\"),\n",
    "                            \"backRev\":  config[3].replace(\"backRev\", \"\"),\n",
    "                            \"numRefLim\": config[4].replace(\"numRefLim\", \"\"),\n",
    "                            \"matches\": matches.array,\n",
    "                            \"sum_matches\": matches.sum(),\n",
    "                            \"nGoldPassages\": numberGoldPassages.sum(),\n",
    "                            \"recall@k\": recall.sum() / len(recall) if len(recall) > 0 else 0\n",
    "                        }\n",
    "                    ]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    return context_id_matches\n",
    "\n",
    "\n",
    "# Set style\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.5)\n",
    "sns.set_palette(\"colorblind\")  # Suitable for colorblind readers\n",
    "sns.color_palette(\"muted\")    # Balanced colors\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# Set configuration\n",
    "parameter = \"rerank\" # pool, quExp, rerank, cExp, backRev, numRefLim\n",
    "dataset = \"miniBiosQA\" # miniBiosQA, miniWiki\n",
    "maxSources = 5\n",
    "#eval_results_dir = f\"../parallel_100_rows_eval/{dataset}/{evaluator}\"\n",
    "pipe_results_dir = f\"../pipe_results/{dataset}/{parameter}\"\n",
    "\n",
    "# Create the dataframe\n",
    "context_matches_df = create_df_goldPassages(dataset, pipe_results_dir)\n",
    "print(context_matches_df)\n",
    "\n",
    "\n",
    "\n",
    "# Select rows where numRefLim is 5 and cExp is False\n",
    "context_matches_df = context_matches_df[(context_matches_df[\"numRefLim\"] == str(maxSources))  & (context_matches_df[\"quExp\"] == \"1\")]\n",
    "print(context_matches_df)\n",
    "context_matches_df = context_matches_df[[\"rerank\", \"sum_matches\",\"nGoldPassages\",\"recall@k\"]]\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Make a bar plot with the sum of matches\n",
    "print(context_matches_df)\n",
    "sns.set_palette(\"colorblind\") \n",
    "sns.barplot(data=context_matches_df, x=\"rerank\", y=\"sum_matches\",palette=\"colorblind\").set(xlabel=\"\")\n",
    "rerank_labels = [\"Off\", \"Semantic\", \"RRF\"]\n",
    "plt.xticks(ticks=range(len(rerank_labels)), labels=rerank_labels)\n",
    "plt.savefig(f\"../plots/{dataset}_rerank_sum_matches_test.pdf\",format=\"pdf\")\n",
    "#plt.title(f\"Sum of gold resource matches {dataset}\")\n",
    "\n",
    "# Make a bar plot with the recall\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=context_matches_df, x=\"rerank\", y=\"recall@k\",palette=\"colorblind\")\n",
    "rerank_labels = [\"Off\", \"Semantic\", \"RRF\"]\n",
    "plt.xticks(ticks=range(len(rerank_labels)), labels=rerank_labels)\n",
    "plt.ylim(0.0,0.5)\n",
    "plt.savefig(f\"../plots/{dataset}_rerank_recall5_test.pdf\",format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get answer and ground_truth of rag elements\n",
    "from evaluate import ROUGE\n",
    "from pprint import pprint\n",
    "answer = \"yes\"\n",
    "ground_truth = \"yes, however the weather is not good and aspiring to be a good person is not easy. Whis leads to some people being bad.\"\n",
    "\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"Ground truth: {ground_truth}\")\n",
    "rouge = ROUGE(answer, ground_truth)\n",
    "pprint(rouge)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at filenames \n",
    "# Get eval results file names\n",
    "eval_results_dir = \"../eval_results/LLMs/miniWiki\"\n",
    "eval_results_file_names = get_csv_files_from_dir(eval_results_dir)\n",
    "\n",
    "# Loop over all eval results files and output the filenames with an index in the dir\n",
    "for i, filename in enumerate(eval_results_file_names[:]):  # Iterate over all eval results files\n",
    "    print(f\"{i}: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Triad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_triad_column(df,combination): \n",
    "    ## Add a column with the triad sum based on the desired combination\n",
    "    if combination == \"average\":\n",
    "        # Sum the CR, Faithfulness and AR values and put in a new colum called triad sum\n",
    "        df[\"CR\"] = df[\"CR\"].apply(lambda x: sum(map(float, x.strip('[]').split())) / len(x.strip('[]').split()) if pd.notna(x) else np.nan)\n",
    "        df[\"Faithfulness\"] = df[\"Faithfulness\"].apply(lambda x: sum(map(float, x.strip('[]').split())) / len(x.strip('[]').split()) if pd.notna(x) else np.nan)\n",
    "        df[\"TriadSum\"] = (df[\"CR\"] + df[\"Faithfulness\"] + df[\"AR\"]) / 3\n",
    "\n",
    "    # Take the first element of context relevance, faithfulness and answer relevance\n",
    "    if combination == \"first\":\n",
    "        print(f\"Linear combination: {combination}\")\n",
    "        df[\"CR\"] = df[\"CR\"].apply(lambda x: float(x.strip('[]').split()[0]))\n",
    "        df[\"Faithfulness\"] = df[\"Faithfulness\"].apply(lambda x: float(x.strip('[]').split()[0]))\n",
    "        df[\"TriadSum\"] = ( df[\"CR\"]* 2 + df[\"Faithfulness\"]* 2 + df[\"AR\"] ) / 3\n",
    "    \n",
    "    if combination == \"high\":\n",
    "        print(f\"Linear combination: {combination}\")\n",
    "        df[\"CR\"] = df[\"CR\"].apply(lambda x: max(map(float, x.strip('[]').split())))\n",
    "        df[\"Faithfulness\"] = df[\"Faithfulness\"].apply(lambda x: max(map(float, x.strip('[]').split())))\n",
    "        df[\"TriadSum\"] = ( df[\"CR\"] + df[\"Faithfulness\"] + df[\"AR\"] ) / 3\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Style\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.5)\n",
    "sns.set_palette(\"viridis\")  # Suitable for colorblind readers\n",
    "sns.color_palette(\"muted\")    # Balanced colors\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "\n",
    "# Get eval results file names\n",
    "eval_results_dir = \"../eval_results/triad\"\n",
    "eval_results_file_names = get_csv_files_from_dir(eval_results_dir)\n",
    "\n",
    "# Loop over all eval results files and output the filenames with an index in the dir\n",
    "for filename in eval_results_file_names[3:4]:  # Iterate over all eval results files\n",
    "    # Filter files: Only look at files with quExp1_rerank1_cExp*_backRevFalse_numRef4\n",
    "    # first_file = f\"{eval_results_dir}/{eval_results_file_names[1]}\"  # Slice for dev\n",
    "    file = eval_results_dir + \"/\" + filename  # Slice for dev\n",
    "    print(f\"File: {file}\")\n",
    "    # Read eval results from CSV\n",
    "    eval_results_df = pd.read_csv(file)\n",
    "    # Get first 10 rows \n",
    "    # eval_results_df = eval_results_df.head(10)\n",
    "    # print(f\"Eval results: {eval_results_df.head()}\")\n",
    "    eval_results_df = add_triad_column(df=eval_results_df,combination=\"high\")\n",
    "\n",
    "\n",
    "    # print(f\" eval results columns: {eval_results_df.columns}\")    \n",
    "    eval_results_df[[\"answer\",'ground_truth',\"TriadSum\",\"CR\",\"Faithfulness\",\"AR\", \"Correct\"]]\n",
    "    corr_matrix = eval_results_df[[\"TriadSum\",\"CR\",\"Faithfulness\",\"AR\", \"Correct\"]].corr(\"pearson\")\n",
    "    print(corr_matrix)\n",
    "\n",
    "\n",
    "##\n",
    "## Examine correlation between TriadSum and Correct\n",
    "## \n",
    "\n",
    "# Heatmap\n",
    "cmap = sns.diverging_palette(220, 20, as_cmap=True)  # 220 for blue, 20 for red\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', linewidths=0.5)\n",
    "plt.savefig(f\"../plots/corr_matrix_test.pdf\",format=\"pdf\")\n",
    "plt.show()\n",
    "##\n",
    "\n",
    "\n",
    "# Pair plot to visualize relationships between variables\n",
    "#sns.pairplot(eval_results_df[['TriadSum','CR','Faithfulness','AR','Correct']])\n",
    "# plt.suptitle('Pair Plot of Variables', y=1.02)\n",
    "\n",
    "#plt.savefig(f\"../plots/pair_plot_test.pdf\",format=\"pdf\")\n",
    "#plt.show()\n",
    "##\n",
    "\n",
    "\n",
    "# Scatterplot with regression line\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "for i, col in enumerate(['TriadSum', 'CR', 'Faithfulness', 'AR']):\n",
    "    sns.regplot(x=eval_results_df[col], y=eval_results_df[\"Correct\"], ax=axes[i], line_kws={\"color\": \"red\"})\n",
    "    axes[i].set_title(f'Scatterplot of {col} vs Correctness')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"../plots/scatter_plot_test.pdf\",format=\"pdf\")\n",
    "plt.show()\n",
    "##\n",
    "\n",
    "# Import additional library\n",
    "from collections import Counter\n",
    "\n",
    "# Prepare the figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(['TriadSum', 'CR', 'Faithfulness', 'AR']):\n",
    "    # Count the frequency of each data point\n",
    "    freq = Counter(zip(eval_results_df[col], eval_results_df[\"Correct\"]))\n",
    "    \n",
    "    # Extract unique x and y values and their frequencies\n",
    "    unique_data = list(freq.keys())\n",
    "    frequencies = list(freq.values())\n",
    "    \n",
    "    x_values, y_values = zip(*unique_data)\n",
    "    \n",
    "    # Plot scatter with size based on frequency\n",
    "    scatter = axes[i].scatter(x_values, y_values, s=[f * 3 for f in frequencies], alpha=0.6, edgecolors='w', c='blue')\n",
    "    \n",
    "    # Add regression line\n",
    "    sns.regplot(x=eval_results_df[col], y=eval_results_df[\"Correct\"], ax=axes[i], \n",
    "                scatter=False, line_kws={\"color\": \"red\"})\n",
    "    \n",
    "    \n",
    "    axes[i].set_xlabel(col,fontsize=12)\n",
    "    axes[i].set_ylabel('Correctness',fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../plots/scatter_plot.pdf\", format=\"pdf\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_koenigsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
