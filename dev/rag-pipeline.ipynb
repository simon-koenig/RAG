{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to develop rag evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define API ENDPOINTS \n",
    "LLM_URL=\"http://10.103.251.104:8040/v1\"\n",
    "LLM_NAME=\"mixtral\"\n",
    "MARQO_URL=\"http://10.103.251.104:8882\"\n",
    "# Old Marqo endpoint; version 1.5\n",
    "# MARQO_URL=\"http://10.103.251.100:8882\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import marqo\n",
    "import re\n",
    "import os\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,  # need to install langchain\n",
    "    NLTKTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import pprint\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from components import VectorStore, RagPipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Connect to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'indexName': 'miniwikiindex'}, {'indexName': 'ait-qm'}]\n",
      "Index connected: miniwikiindex \n"
     ]
    }
   ],
   "source": [
    "# Set Index Settings, docs: https://docs.marqo.ai/2.5/API-Reference/Indexes/create_index/\n",
    "INDEX_NAME = \"mini_wiki_index\"\n",
    "index_params = {\n",
    "    \"split_method\": \"sentence\",\n",
    "    \"distance_metric\": \"prenormalized-angular\",\n",
    "    \"model\": \"hf/all_datasets_v4_MiniLM-L6\",\n",
    "    #\"model\" : 'flax-sentence-embeddings/all_datasets_v4_mpnet-base',\n",
    "}\n",
    "\n",
    "documentDB = VectorStore(MARQO_URL) # Connect to marqo client via python API\n",
    "#documentDB.createIndex(\"miniWikiIndex\", index_params) # Create index with name miniWikiIndex\n",
    "#print(documentDB.getIndexes()) # Print all indexes\n",
    "#documentDB.deleteIndex(\"mini_wiki_index\") # Delete index with name miniWikiIndex\n",
    "print(documentDB.getIndexes()) # Print all indexes\n",
    "documentDB.connectIndex(\"miniwikiindex\") # Connect to index with name miniWikiIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Uruguay (official full name in  ; pron.  , Eastern Republic of  Uruguay) is a country located in the southeastern part of South America.  It is home to 3.3 million people, of which 1.7 million live in the capital Montevideo and its metropolitan area.', 'id': 0}\n"
     ]
    }
   ],
   "source": [
    "# Code to prepare dataset\n",
    "def prepare_mini_wiki(corpus, chunking_params):\n",
    "    # Upload a mini wiki corpus to the marqo instance\n",
    "    # The corpus is a dictionary with two keys. Passages and id. \n",
    "    # Passage is a list of strings \n",
    "    # Id is a list of ints.\n",
    "    # Open the PDF file\n",
    "\n",
    "    # Create a list of dictionaries with keys: passage, id\n",
    "    corpus_list = []\n",
    "    for passage, iD in zip(corpus[\"passage\"], corpus[\"id\"]):\n",
    "            corpus_list.append({\"text\": passage, \"id\": iD})\n",
    "    return corpus_list\n",
    "\n",
    "# Prepare the mini wiki corpus\n",
    "chunking_params = {\n",
    "    \"chunk_size\": 1024,\n",
    "    \"chunk_overlap\": 128,\n",
    "    \"chunk_method\": \"recursive\",\n",
    "}\n",
    "\n",
    "\n",
    "mini_wiki_corpus = load_dataset(\"rag-datasets/mini_wikipedia\", \"text-corpus\")\n",
    "passages = mini_wiki_corpus[\"passages\"]\n",
    "corpus_list = prepare_mini_wiki(passages, chunking_params)\n",
    "print(corpus_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to upload 100 passages: 7.095343112945557 seconds\n",
      "{'numberOfDocuments': 200, 'numberOfVectors': 334, 'backend': {'memoryUsedPercentage': 0.08232219977, 'storageUsedPercentage': 30.8928638317}}\n"
     ]
    }
   ],
   "source": [
    "# Code to Index data\n",
    "upload_start = time.time()\n",
    "maxDocs = 100\n",
    "documentDB.indexDocuments(documents=corpus_list, maxDocs=maxDocs)\n",
    "upload_end = time.time()\n",
    "print(f\"Time taken to upload {min(len(corpus_list), maxDocs)} passages: {upload_end - upload_start} seconds\")\n",
    "# Check if index contains data\n",
    "print(documentDB.getIndexStats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we have the documents indexed. Now retrieve passages based on a query. \n",
    "Indexing 3200 short passages takes ~ 4minutes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get questions and answers from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': ['Was Abraham Lincoln the sixteenth President of the United States?', 'Did Lincoln sign the National Banking Act of 1863?', 'Did his mother die of pneumonia?', \"How many long was Lincoln's formal education?\", 'When did Lincoln begin his political career?'], 'answer': ['yes', 'yes', 'no', '18 months', '1832'], 'id': [0, 2, 4, 6, 8]}\n",
      "['Was Abraham Lincoln the sixteenth President of the United States?', 'Did Lincoln sign the National Banking Act of 1863?', 'Did his mother die of pneumonia?', \"How many long was Lincoln's formal education?\", 'When did Lincoln begin his political career?']\n",
      "['yes', 'yes', 'no', '18 months', '1832']\n"
     ]
    }
   ],
   "source": [
    "# Get question - answer - (passages) dataset\n",
    "mini_wiki_qa = load_dataset(\"rag-datasets/mini_wikipedia\", \"question-answer\")\n",
    "mini_wiki_qa = mini_wiki_qa[\"test\"][0:5] # Load the first 5 qestion-answer-id triples\n",
    "print(mini_wiki_qa)\n",
    "questions = mini_wiki_qa[\"question\"]\n",
    "answers = mini_wiki_qa[\"answer\"]\n",
    "print(questions)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline and connect to vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Language model URL: http://10.103.251.104:8040/v1\n",
      " Language model connected: mixtral\n",
      "[\"Montevideo, Uruguay's capital.\",\n",
      " \"Montevideo, Uruguay's capital.\",\n",
      " 'Map of Uruguay']\n",
      "Sending query to OpenAI endpoint: http://10.103.251.104:8040/v1/chat/completions\n",
      "Received response...\n",
      "Response: \n",
      "\n",
      "(' The capital of Uruguay is Montevideo. It is the largest city in the country '\n",
      " 'and serves as the political, economic, and cultural hub of Uruguay. '\n",
      " 'Montevideo is known for its vibrant cultural scene, beautiful beaches, and '\n",
      " 'historic architecture, including the iconic Salvo Palace and the '\n",
      " 'Metropolitan Cathedral. The city has a rich history, having been founded in '\n",
      " \"1726 by Spanish settlers, and it played an important role in the country's \"\n",
      " 'struggle for independence from Spain in the early 19th century. Today, '\n",
      " 'Montevideo is a popular tourist destination and a thriving center of '\n",
      " 'commerce and industry in Uruguay.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" The capital of Uruguay is Montevideo. It is the largest city in the country and serves as the political, economic, and cultural hub of Uruguay. Montevideo is known for its vibrant cultural scene, beautiful beaches, and historic architecture, including the iconic Salvo Palace and the Metropolitan Cathedral. The city has a rich history, having been founded in 1726 by Spanish settlers, and it played an important role in the country's struggle for independence from Spain in the early 19th century. Today, Montevideo is a popular tourist destination and a thriving center of commerce and industry in Uruguay.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pipeline object \n",
    "pipe = RagPipe()\n",
    "pipe.connectVectorStore(documentDB)\n",
    "pipe.connectLLM(LLM_URL, LLM_NAME)\n",
    "pipe.answerQuery(\"What is the capital of Uruguay?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'unstructured',\n",
       " 'treatUrlsAndPointersAsImages': False,\n",
       " 'filterStringMaxLength': 20,\n",
       " 'model': 'hf/all_datasets_v4_MiniLM-L6',\n",
       " 'normalizeEmbeddings': True,\n",
       " 'textPreprocessing': {'splitLength': 2,\n",
       "  'splitOverlap': 0,\n",
       "  'splitMethod': 'sentence'},\n",
       " 'imagePreprocessing': {},\n",
       " 'vectorNumericType': 'float',\n",
       " 'annParameters': {'spaceType': 'prenormalized-angular',\n",
       "  'parameters': {'efConstruction': 512, 'm': 16}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test cell \n",
    "# Delete all docs in index\n",
    "documentDB.getIndexStats()\n",
    "#documentDB.emptyIndex()\n",
    "documentDB.getIndexSettings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 36\n",
      "Context: Montevideo, Uruguay's capital.\n",
      "ID: 36\n",
      "Context: Montevideo, Uruguay's capital.\n",
      "ID: 28\n",
      "Context: Map of Uruguay\n",
      "ID: 80\n",
      "Context: Michael Faraday was born in Newington Butts, near present-day South London, England. His family was not well off. His father, James, was a member of the Sandemanian sect of Christianity. James Faraday had come to London ca 1790 from Outhgill in Westmorland, where he had been the village blacksmith. The young Michael Faraday, one of four children, having only the most basic of school educations, had to largely educate himself. \"Michael Faraday.\"  History of Science and Technology. Houghton Mifflin Company, 2004. Answers.com 4 June 2007.  /ref> At fourteen he became apprenticed to a local bookbinder and bookseller George Riebau and, during his seven-year apprenticeship, he read many books, including Isaac Watts' The Improvement of the Mind, and he enthusiastically implemented the principles and suggestions contained therein. He developed an interest in science and specifically in electricity. In particular, he was inspired by the book Conversations in Chemistry by Jane Marcet.\n",
      "ID: 80\n",
      "Context: Michael Faraday was born in Newington Butts, near present-day South London, England. His family was not well off. His father, James, was a member of the Sandemanian sect of Christianity. James Faraday had come to London ca 1790 from Outhgill in Westmorland, where he had been the village blacksmith. The young Michael Faraday, one of four children, having only the most basic of school educations, had to largely educate himself. \"Michael Faraday.\"  History of Science and Technology. Houghton Mifflin Company, 2004. Answers.com 4 June 2007.  /ref> At fourteen he became apprenticed to a local bookbinder and bookseller George Riebau and, during his seven-year apprenticeship, he read many books, including Isaac Watts' The Improvement of the Mind, and he enthusiastically implemented the principles and suggestions contained therein. He developed an interest in science and specifically in electricity. In particular, he was inspired by the book Conversations in Chemistry by Jane Marcet.\n",
      "ID: 2\n",
      "Context: Montevideo was founded by the Spanish in the early 18th century as a military stronghold. Uruguay won its independence in 1828 following a three-way struggle between Spain, Argentina and Brazil. It is a constitutional democracy, where the president fulfills the roles of both head of state and head of government\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'What is the capital of Uruguay?': [36, 36, 28],\n",
       " 'Where is Washington?': [80, 80, 2]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Analyse Search Results - CONTEXT RELEVANCE\n",
    "def evaluate_context_relevance(queries, dataBase , k, goldPassages=None, ):\n",
    "    # Get retrieved text.\n",
    "    # Get retrieved text.\n",
    "    scores = {}\n",
    "    for query in queries:\n",
    "        response = dataBase.retrieveDocuments(query=query, k=k)\n",
    "        contexts = [response[\"hits\"][i][\"text\"] for i in range(len(response[\"hits\"]))]\n",
    "        ids = [response[\"hits\"][i][\"id\"] for i in range(len(response[\"hits\"]))]\n",
    "        measurements = []\n",
    "        for id, context in zip(ids,contexts):\n",
    "            # Insert here evaluation measure of retrieved context\n",
    "            print(f\"ID: {id}\")\n",
    "            print(f\"Context: {context}\")\n",
    "            measure = id # Insert evaluation measure here\n",
    "            measurements.append(measure)\n",
    "        \n",
    "        scores[query] = measurements # Insert evaluation measure here\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "queries = [\"What is the capital of Uruguay?\", \"Where is Washington?\"] # Query\n",
    "evaluate_context_relevance(queries, documentDB, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending query to OpenAI endpoint: http://10.103.251.104:8040/v1/chat/completions\n",
      "Received response...\n",
      "Response: \n",
      "\n",
      "' 1'\n"
     ]
    }
   ],
   "source": [
    "# Test llm judge on rating the relevance of a context\n",
    "context = (\"Uruguay's capital, Montevideo, was founded by the Spanish in the early 18th century \"\n",
    "\"as a military stronghold; its natural \"\n",
    "\"harbor soon developed into a commercial center competing with Argentina's capital, Buenos Aires.\")\n",
    "context1 = \"Urugays largest city is Montevideo\"\n",
    "query = \"What is the capital of Uruguay?\"\n",
    "\n",
    "def llm_binary_context_relevance(context, query, LLM_NAME, LLM_URL):\n",
    "    messages = [\n",
    "                {\"role\": \"system\", \"content\": \"Given the following context and query,\"\n",
    "                \" Give a binary rating, either 0 or 1.\"\n",
    "                \" 0 means the context is not sufficient for answering the query. \"\n",
    "                \" 1 means the context is sufficient for answering the query. \"\n",
    "                \".Respond with a single integer and give no additional explaination. \"\n",
    "                'The output must strictly be \"0\" or \"1\"' },\n",
    "                {\"role\": \"user\", \"content\": f\"Context: {context1} ; Query: {query}\"}\n",
    "            ]\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer N/A \",\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": LLM_NAME,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 2,\n",
    "        # \"presence_penalty\": presence_pen,\n",
    "        # \"repeat_penalty\": repeat_pen,\n",
    "    }\n",
    "    endpoint = LLM_URL + \"/chat/completions\"\n",
    "    print(\"Sending query to OpenAI endpoint: \" + endpoint)\n",
    "    report = requests.post(endpoint, headers=headers, json=data).json()\n",
    "    print(\"Received response...\")\n",
    "    if \"choices\" in report:\n",
    "        if len(report[\"choices\"]) > 0:  # Always take the first choice.\n",
    "            result = report[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            result = \"No result generated!\"\n",
    "    else:\n",
    "        result = report\n",
    "    print(\"Response: \\n\")\n",
    "    pprint.pprint(result)\n",
    "    return result\n",
    "score = llm_binary_context_relevance(context, query, LLM_NAME, LLM_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending query to OpenAI endpoint: http://10.103.251.104:8040/v1/chat/completions\n",
      "Received response...\n",
      "Response: \n",
      "\n",
      "' 0'\n"
     ]
    }
   ],
   "source": [
    "# Test llm judge on rating the faithfullness of a context and answer\n",
    "context = (\"Uruguay's capital, Montevideo, was founded by the Spanish in the early 18th century \"\n",
    "\"as a military stronghold; its natural \"\n",
    "\"harbor soon developed into a commercial center competing with Argentina's capital, Buenos Aires.\")\n",
    "context1 = \"Urugays largest city is Montevideo\"\n",
    "answer = \"The capital of uraguay is Bueonos Aires\"\n",
    "\n",
    "def llm_binary_faithfullness(context, answer, LLM_NAME, LLM_URL):\n",
    "    messages = [\n",
    "                {\"role\": \"system\", \"content\": \"Given the following context and answer,\"\n",
    "                \" Give a binary rating, either 0 or 1.\"\n",
    "                \" 0 means the answer is not sufficiently grounded in the context. \"\n",
    "                \" 1 means the answer is sufficiently grounded in the context \"\n",
    "                \".Respond with a single integer and give no additional explaination. \"\n",
    "                'The output must strictly be \"0\" or \"1\"' },\n",
    "                {\"role\": \"user\", \"content\": f\"Context: {context1} ; Answer: {answer}\"}\n",
    "            ]\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer N/A \",\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": LLM_NAME,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 2,\n",
    "        # \"presence_penalty\": presence_pen,\n",
    "        # \"repeat_penalty\": repeat_pen,\n",
    "    }\n",
    "    endpoint = LLM_URL + \"/chat/completions\"\n",
    "    print(\"Sending query to OpenAI endpoint: \" + endpoint)\n",
    "    report = requests.post(endpoint, headers=headers, json=data).json()\n",
    "    print(\"Received response...\")\n",
    "    if \"choices\" in report:\n",
    "        if len(report[\"choices\"]) > 0:  # Always take the first choice.\n",
    "            result = report[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            result = \"No result generated!\"\n",
    "    else:\n",
    "        result = report\n",
    "    print(\"Response: \\n\")\n",
    "    pprint.pprint(result)\n",
    "    return result\n",
    "score = llm_binary_faithfullness(context, answer, LLM_NAME, LLM_URL)\n",
    "\n",
    "\n",
    "# 2. Analyse if answer built upon search results  - FAITHFULNESS\n",
    "def evaluate_faithfulness(answers, contexts):\n",
    "    scores = {}\n",
    "    for answer in answers:\n",
    "        measurements = []\n",
    "        for context in contexts:\n",
    "            # Insert here evaluation measure of retrieved context\n",
    "            print(f\"ID: {id}\")\n",
    "            print(f\"Context: {context}\")\n",
    "            measure = id # Insert evaluation measure here\n",
    "            measurements.append(measure)\n",
    "\n",
    "        scores[query] = measurements # Insert evaluation measure here\n",
    "\n",
    "    return scores\n",
    "#faithfulness_scores = evaluate_faithfulness(answer, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending query to OpenAI endpoint: http://10.103.251.104:8040/v1/chat/completions\n",
      "Received response...\n",
      "Response: \n",
      "\n",
      "' 0'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 0'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Analyse if answer is relevant to the question - ANSWER RELEVANCE\n",
    "def evaluate_answer_relevance(answers, queries):\n",
    "    scores = []\n",
    "    for answer,query in zip(answers,queries):\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        measure = 42 # Insert evaluation measure here\n",
    "        scores.append(measure)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def llm_binary_answer_relevance(answer,query, LLM_NAME, LLM_URL):\n",
    "    messages = [\n",
    "                {\"role\": \"system\", \"content\": \"Given the following query and answer,\"\n",
    "                \" Give a binary rating, either 0 or 1.\"\n",
    "                \" 0 means the answer is not sufficient in answering the question\"\n",
    "                \" 1 means the answer is sufficient in answering the question\"\n",
    "                \".Respond with a single integer and give no additional explaination. \"\n",
    "                'The output must strictly be \"0\" or \"1\"' },\n",
    "                {\"role\": \"user\", \"content\": f\"Query: {query} ; Answer: {answer}\"}\n",
    "            ]\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer N/A \",\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": LLM_NAME,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 2,\n",
    "        # \"presence_penalty\": presence_pen,\n",
    "        # \"repeat_penalty\": repeat_pen,\n",
    "    }\n",
    "    endpoint = LLM_URL + \"/chat/completions\"\n",
    "    print(\"Sending query to OpenAI endpoint: \" + endpoint)\n",
    "    report = requests.post(endpoint, headers=headers, json=data).json()\n",
    "    print(\"Received response...\")\n",
    "    if \"choices\" in report:\n",
    "        if len(report[\"choices\"]) > 0:  # Always take the first choice.\n",
    "            result = report[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            result = \"No result generated!\"\n",
    "    else:\n",
    "        result = report\n",
    "    print(\"Response: \\n\")\n",
    "    pprint.pprint(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "question = \"Who is the best basketball player ever. \"\n",
    "answer1 = \"The best basketball player ever is Michael Jordan.\"\n",
    "answer2 = \"The best football player ever is Lionel Messi\"\n",
    "llm_binary_answer_relevance(answer2, question, LLM_NAME, LLM_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875a9f2a95cb4b049cc2050853481009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7524eb9365b421f89c141fa0e122af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd3cf97420d45c2bfe2a8dc258750f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6b3f910174453ab2f126b3d475c05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf3d46479d940029f42966478a9696e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9b3d75b1664a58a82b0bb50016c4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb42143482c548cf9738cedd94368044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a66adc8ef94ee4818b6d6dda5be7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073c3d37c0e749708400164a99f17ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097770b447b843779332789a392a5060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a041ee727e6846458fe4f2b7869f7855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Similarity Score = tensor([[0.8866]]) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.8866]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Analyse if answer is correct - ANSWER CORRECTNESS\n",
    "from scipy.spatial import distance\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def evaluate_correctness(answers,ground_truths):\n",
    "    scores = []\n",
    "    for answer,query in zip(answers,ground_truths):\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(f\"Query: {ground_truth}\")\n",
    "        measure = 42 # Insert evaluation measure here\n",
    "        scores.append(measure)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def semantic_similarity(sentence1,sentence2):\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "    sentence1_vec = model.encode([sentence1])\n",
    "\n",
    "    sentence2_vec = model.encode([sentence2])\n",
    "    similarity_score = model.similarity(sentence1_vec, sentence2_vec) # Default is cosine simi\n",
    "    print(f'\\n Similarity Score = {similarity_score} ')\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "answer = \"The best basketball player ever is Stephen Curry.\"\n",
    "ground_truth = \"The best basketball player ever is Michael Jordan.\"\n",
    "semantic_similarity(answer,ground_truth)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_koenigsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
