{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to develop rag evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define API ENDPOINTS \n",
    "LLM_URL=\"http://10.103.251.104:8040/v1\"\n",
    "LLM_NAME=\"llama3\"\n",
    "MARQO_URL=\"http://10.103.251.104:8882\"\n",
    "# Old Marqo endpoint; version 1.5\n",
    "# MARQO_URL=\"http://10.103.251.100:8882\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import marqo\n",
    "import re\n",
    "import os\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,  # need to install langchain\n",
    "    NLTKTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import pprint\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from components import VectorStore, RagPipe, DatasetHelpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for a single rag element\n",
    "example_entry = {\n",
    "    \"question\": \"What is the capital of France?\",\n",
    "    \"answer\": \"Paris\",\n",
    "    \"contexts\": [\n",
    "        \"Paris is the capital of France and a major European city.\",\n",
    "        \"Paris is located on the River Seine in northern France.\",\n",
    "        \"Marseille is known for its art, culture, and landmarks.\"\n",
    "    ],\n",
    "    \"context_ids\": [\"1\", \"2\", \"3\"],\n",
    "    \"ground_truth\": \"The capital of France is Paris.\"\n",
    "}\n",
    "\n",
    "print(example_entry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MiniWiki dataset\n",
      "[{'indexName': 'miniwikiindex'}, {'indexName': 'ait-qm'}]\n",
      "Index connected: miniwikiindex \n",
      " Language model URL: http://10.103.251.104:8040/v1\n",
      " Language model connected: llama3\n",
      "Using already indexed documents\n",
      "Index Stats:  {'numberOfDocuments': 10, 'numberOfVectors': 15, 'backend': {'memoryUsedPercentage': 0.08814318706999999, 'storageUsedPercentage': 30.90243340992}}\n",
      "Start answering queries. Please wait. \n",
      "Current Question: Was Abraham Lincoln the sixteenth President of the United States?\n",
      "Sending query to OpenAI endpoint: http://10.103.251.104:8040/v1/chat/completions\n",
      "Received response...\n",
      "Current Question: Did Lincoln sign the National Banking Act of 1863?\n",
      "Sending query to OpenAI endpoint: http://10.103.251.104:8040/v1/chat/completions\n",
      "Received response...\n",
      "Current Question: Did his mother die of pneumonia?\n",
      "Sending query to OpenAI endpoint: http://10.103.251.104:8040/v1/chat/completions\n",
      "Received response...\n"
     ]
    }
   ],
   "source": [
    "# Run eval in a few lines\n",
    "\n",
    "# Load the dataset\n",
    "dataset = DatasetHelpers()\n",
    "corpus_list, queries, ground_truths = dataset.loadMiniWiki()\n",
    "\n",
    "# Load the VectorStore\n",
    "documentDB = VectorStore(MARQO_URL) # Connect to marqo client via python API\n",
    "print(documentDB.getIndexes()) # Print all indexes\n",
    "documentDB.connectIndex(\"miniwikiindex\") # Connect to the miniwikiindex\n",
    "\n",
    "# Load the RagPipe\n",
    "pipe = RagPipe()\n",
    "pipe.connectVectorStore(documentDB)\n",
    "pipe.connectLLM(LLM_URL, LLM_NAME)\n",
    "\n",
    "# Run the rag pipeline and ingest\n",
    "pipe.run(queries,ground_truths, corpus_list,newIngest=False,maxDocs=10,maxQueries=3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: A very easy one!\n",
      "\n",
      "Yes, Abraham Lincoln was indeed the 16th President of the United States! He served from March 1861 until his assassination in April 1865.\n",
      "Ground truth: yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/master_project/software/venv_koenigsi/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Similarity Score = tensor([[0.1328]]) \n",
      "Answer: No, Abraham Lincoln did not sign the National Banking Act of 1863.\n",
      "\n",
      "The National Banking Act was actually signed into law by President Andrew Johnson on February 25, 1865. This act established a national banking system in the United States and created a new type of bank charter called a \"national bank.\" The act also required national banks to invest a certain percentage of their capital in U.S. government securities.\n",
      "\n",
      "Abraham Lincoln was assassinated on April 14, 1865, and died the next morning, so he did not have an opportunity to sign this legislation into law.\n",
      "Ground truth: yes\n",
      "\n",
      " Similarity Score = tensor([[0.1200]]) \n",
      "Answer: I apologize, but this prompt doesn't seem to be related to the previous conversation about Uruguay. Could you please provide more context or clarify what you would like to know about someone's mother passing away from pneumonia? I'll do my best to help!\n",
      "Ground truth: no\n",
      "\n",
      " Similarity Score = tensor([[0.0170]]) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[0.1328]]), tensor([[0.1200]]), tensor([[0.0170]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the rag pipeline\n",
    "pipe.eval(method=\"correctness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from an example dataset\n",
    "# Code to prepare dataset\n",
    "def prepare_mini_wiki(corpus, chunking_params):\n",
    "    # Upload a mini wiki corpus to the marqo instance\n",
    "    # The corpus is a dictionary with two keys. Passages and id. \n",
    "    # Passage is a list of strings \n",
    "    # Id is a list of ints.\n",
    "    # Open the PDF file\n",
    "\n",
    "    # Create a list of dictionaries with keys: passage, id\n",
    "    corpus_list = []\n",
    "    for passage, iD in zip(corpus[\"passage\"], corpus[\"id\"]):\n",
    "            corpus_list.append({\"text\": passage, \"id\": iD})\n",
    "    return corpus_list\n",
    "\n",
    "# Prepare the mini wiki corpus\n",
    "chunking_params = {\n",
    "    \"chunk_size\": 1024,\n",
    "    \"chunk_overlap\": 128,\n",
    "    \"chunk_method\": \"recursive\",\n",
    "}\n",
    "\n",
    "\n",
    "mini_wiki_corpus = load_dataset(\"rag-datasets/mini_wikipedia\", \"text-corpus\")[\"passages\"]\n",
    "corpus_list = prepare_mini_wiki(mini_wiki_corpus, chunking_params)\n",
    "print(corpus_list[0])\n",
    "\n",
    "# Get question - answer - (passages) dataset\n",
    "mini_wiki_qa = load_dataset(\"rag-datasets/mini_wikipedia\", \"question-answer\")[\"test\"][0:5] # Load the first 5 qestion-answer-id triples\n",
    "print(mini_wiki_qa)\n",
    "questions = mini_wiki_qa[\"question\"]\n",
    "ground_truths = mini_wiki_qa[\"answer\"]\n",
    "print(questions)\n",
    "print(ground_truths)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## 1. Connect to Vector database\n",
    "## \n",
    "\n",
    "documentDB = VectorStore(MARQO_URL) # Connect to marqo client via python API\n",
    "print(documentDB.getIndexes()) # Print all indexes\n",
    "documentDB.connectIndex(\"miniwikiindex\") # Connect to index with name miniWikiIndex\n",
    "documentDB.emptyIndex()\n",
    "\n",
    "\n",
    "##\n",
    "## 2. Index Documents\n",
    "##\n",
    "mini_wiki_corpus = load_dataset(\"rag-datasets/mini_wikipedia\", \"text-corpus\")[\"passages\"]\n",
    "corpus_list = prepare_mini_wiki(mini_wiki_corpus, chunking_params)\n",
    "print(corpus_list[0])\n",
    "\n",
    "maxDocs = 1000\n",
    "documentDB.indexDocuments(documents=corpus_list, maxDocs=maxDocs)\n",
    "print(documentDB.getIndexStats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create elemtents for the RAG model. With question and ground truth. \n",
    "# Then appear contexts and llm answers via RAG model\n",
    "\n",
    "# Create pipeline object \n",
    "pipe = RagPipe()\n",
    "pipe.connectVectorStore(documentDB)\n",
    "pipe.connectLLM(LLM_URL, LLM_NAME)\n",
    "\n",
    "\n",
    "# Get question - answer - (passages) dataset\n",
    "mini_wiki_qa = load_dataset(\"rag-datasets/mini_wikipedia\", \"question-answer\")[\"test\"][0:5] # Load the first 5 qestion-answer-id triples\n",
    "\n",
    "# Create a list of dictionaries with keys: question, answer, contexts, context_ids, ground_truth\n",
    "rag_elements = []\n",
    "for question, ground_truth in zip(questions, ground_truths):\n",
    "    rag_elements.append({\"question\": question, \"answer\": \"\", \"contexts\": [], \"context_ids\": [], \"ground_truth\": ground_truth})\n",
    "\n",
    "# Iterate over the rag elements and get the answer from the LLM model and the contexts from the Vector DB\n",
    "for rag_element in rag_elements: \n",
    "    llmanswer, contexts, context_ids = pipe.answerQuery(rag_element[\"question\"]) # Get answer from LLM model\n",
    "    rag_element[\"answer\"] = llmanswer\n",
    "    rag_element[\"contexts\"] = contexts\n",
    "    rag_element[\"context_ids\"] = context_ids\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rag_element in rag_elements:\n",
    "    print(rag_element)\n",
    "    print(\"Question: \", rag_element[\"question\"])\n",
    "    print(\"Answer: \", rag_element[\"answer\"])\n",
    "    print(\"Ground Truth: \", rag_element[\"ground_truth\"])\n",
    "    print(\"Contexts: \")\n",
    "    for context, context_id in zip(rag_element[\"contexts\"], rag_element[\"context_ids\"]):\n",
    "        print(\"Context ID: \", context_id)\n",
    "        print(\"Context: \", context)\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate context relevancen for a single element\n",
    "\n",
    "## \n",
    "## 1. Connect to Vector database\n",
    "## \n",
    "\n",
    "#documentDB = VectorStore(MARQO_URL) # Connect to marqo client via python API\n",
    "#print(documentDB.getIndexes()) # Print all indexes\n",
    "#documentDB.connectIndex(\"miniwikiindex\") # Connect to index with name miniWikiIndex\n",
    "\n",
    "##\n",
    "## Create pipeline object \n",
    "## \n",
    "\n",
    "#pipe = RagPipe()\n",
    "#pipe.connectVectorStore(documentDB)\n",
    "#pipe.connectLLM(LLM_URL, LLM_NAME)\n",
    "\n",
    "# question = example_entry[\"question\"]\n",
    "# contexts = example_entry[\"contexts\"]\n",
    "# contexts_ids = example_entry[\"context_ids\"]\n",
    "# answer = example_entry[\"answer\"]\n",
    "# ground_truth = example_entry[\"ground_truth\"]\n",
    "\n",
    "##\n",
    "## Evaluate example context relevance. Possible to evaluate multiple questions-contexts at once\n",
    "##\n",
    "\n",
    "for rag_element in rag_elements:\n",
    "    question = rag_element[\"question\"]\n",
    "    contexts = rag_element[\"contexts\"]\n",
    "    contexts_ids = rag_element[\"context_ids\"]\n",
    "    scores = pipe.evaluate_context_relevance([question], [contexts], [contexts_ids])\n",
    "    print(\"Context Relevance Scores: \", scores)\n",
    "    \n",
    "# pipe.evaluate_context_relevance([question], [contexts,contexts], [contexts_ids,contexts_ids])\n",
    "\n",
    "\n",
    "## \n",
    "## Evaluate faithfulness. I.e. If answer is faithful to the context\n",
    "##\n",
    "\n",
    "# pipe.evaluate_faithfulness([\"Marseille is a nice city\", \"Paris is in the north of France\"], [contexts, contexts])\n",
    "\n",
    "## \n",
    "## Evaluate answer relevance. I.e. If answer is relevant to the question\n",
    "##\n",
    " \n",
    "# pipe.evaluate_answer_relevance([question], [\"Berlin is the capital of France\"])\n",
    "\n",
    "\n",
    "##\n",
    "## Evaluate answer correctness. I.e. If answer is correct compared to ground truth\n",
    "##\n",
    "\n",
    "# pipe.evaluate_correctness([answer], [ground_truth]) \n",
    "# Need more than just sentence transformer comparison\n",
    "# Either also an LLM judge or use ROUGE or BLEU score in addtion to semantic sentence. \n",
    "# E'g' \"Paris is the capital of France\" and \"Paris\" should be considered a perfect match \n",
    "# but semantic sentence similarity is just 0.56 or so.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_koenigsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
